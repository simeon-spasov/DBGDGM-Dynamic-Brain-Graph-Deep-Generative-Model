{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_nodes = 5\n",
    "dimensions = 3\n",
    "clusters = 3\n",
    "eta = 5.\n",
    "lower_control = 10 ** -15\n",
    "alpha = 0.05\n",
    "lambd = 0.2\n",
    "beta = 0.05\n",
    "\n",
    "A = np.random.randint(0, 2, size=(number_of_nodes, number_of_nodes))\n",
    "A = (((A + A.T) / 2) > 0) * 1.\n",
    "np.fill_diagonal(A, 1.)\n",
    "graph = nx.from_numpy_matrix(A)\n",
    "# np.fill_diagonal(A, 1.)\n",
    "A = torch.from_numpy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _modularity_matrix(graph):\n",
    "    \"\"\"Calculate modularity matrix.\"\"\"\n",
    "    \n",
    "    degs = nx.degree(graph)\n",
    "\n",
    "    num_edges = graph.number_of_edges()\n",
    "    n_count = graph.number_of_nodes()\n",
    "    mod_matrix = np.zeros((n_count, n_count))\n",
    "    \n",
    "    indices_1 = np.array([edge[0] for edge in graph.edges()] + [edge[1] for edge in graph.edges()])\n",
    "    indices_2 = np.array([edge[1] for edge in graph.edges()] + [edge[0] for edge in graph.edges()])\n",
    "\n",
    "    scores = [float(degs[e[0]] * degs[e[1]]) / (2 * num_edges) for e in graph.edges()]\n",
    "    scores += [float(degs[e[1]] * degs[e[0]]) / (2 * num_edges) for e in graph.edges()]\n",
    "\n",
    "    mod_matrix[indices_1, indices_2] = scores\n",
    "\n",
    "    return mod_matrix\n",
    "\n",
    "\n",
    "def modularity_matrix(A):\n",
    "    \"\"\"Calculate modularity matrix.\"\"\"\n",
    "    \n",
    "    degs = A.sum(dim=1) + 1\n",
    "    num_edges = A.tril().sum(dim=(-2, -1))\n",
    "    batch_size = A.shape[0]\n",
    "\n",
    "    edge_indices = torch.stack(A.triu().nonzero().transpose(-2, -1)[1:, ...].tensor_split(batch_size, dim=-1))\n",
    "    row_idx = edge_indices[:, 0, :].view(batch_size, -1).expand(-1, -1)\n",
    "    col_idx = edge_indices[:, 1, :].view(batch_size, -1).expand(-1, -1)\n",
    "\n",
    "    scores_l = (torch.gather(degs, -1, row_idx) * torch.gather(degs, -1, col_idx)) / (2 * num_edges.unsqueeze(-1)) \n",
    "    scores_u = (torch.gather(degs, -1, col_idx) * torch.gather(degs, -1, row_idx)) / (2 * num_edges.unsqueeze(-1)) \n",
    "    \n",
    "    mod_matrix = torch.zeros_like(A)\n",
    "    mod_matrix[:, row_idx, col_idx] = scores_l\n",
    "    mod_matrix[:, col_idx, row_idx] = scores_u\n",
    "\n",
    "    return mod_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "_M = np.random.uniform(0, 1, (number_of_nodes, dimensions))\n",
    "_U = np.random.uniform(0, 1, (number_of_nodes, dimensions))\n",
    "_H = np.random.uniform(0, 1, (number_of_nodes, clusters))\n",
    "_C = np.random.uniform(0, 1, (clusters, dimensions))\n",
    "_B1 = nx.adjacency_matrix(graph, nodelist=range(graph.number_of_nodes()))\n",
    "_B2 = _modularity_matrix(graph)\n",
    "_X = np.transpose(_U)\n",
    "_overlaps = _B1.dot(_B1)\n",
    "_S = _B1 + eta * _B1 * (_overlaps)\n",
    "\n",
    "M = torch.from_numpy(_M).unsqueeze(0).repeat_interleave(2, 0)\n",
    "U = torch.from_numpy(_U).unsqueeze(0).repeat_interleave(2, 0)\n",
    "H = torch.from_numpy(_H).unsqueeze(0).repeat_interleave(2, 0)\n",
    "C = torch.from_numpy(_C).unsqueeze(0).repeat_interleave(2, 0)\n",
    "B1 = A.unsqueeze(0).repeat_interleave(2, 0)\n",
    "B2 = modularity_matrix(A.unsqueeze(0))\n",
    "X = U.transpose(dim0=-2, dim1=-1)\n",
    "overlaps = B1.matmul(B1)\n",
    "S = B1 + eta * B1.matmul(overlaps)\n",
    "\n",
    "print(torch.allclose(B1[0], torch.tensor(_B1.todense())))\n",
    "print(torch.allclose(B2[0], torch.tensor(_B2)))\n",
    "print(torch.allclose(X[0], torch.tensor(_X)))\n",
    "print(torch.allclose(overlaps[0], torch.tensor(_overlaps.todense())))\n",
    "print(torch.allclose(S[0], torch.tensor(_S.todense())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _update_M(_S, _U, _M, lower_control):\n",
    "    _enum = _S.dot(_U)\n",
    "    _denom = np.dot(_M, np.dot(np.transpose(_U), _U))\n",
    "    _denom[_denom < lower_control] = lower_control\n",
    "    _new_M = np.multiply(_M, _enum / _denom)\n",
    "    _row_sums = _new_M.sum(axis=1)\n",
    "    _new_M = _new_M / _row_sums[:, np.newaxis]\n",
    "    return _new_M\n",
    "\n",
    "def update_M(S, U, M, lower_control):\n",
    "    enum = S.matmul(U)\n",
    "    denom = M.matmul(U.transpose(-2, -1).matmul(U))\n",
    "    denom[denom < lower_control] = lower_control\n",
    "    new_M = M * (enum / denom)\n",
    "    row_sums = new_M.sum(dim=-1)\n",
    "    new_M = new_M / row_sums.unsqueeze(-1)\n",
    "    return new_M\n",
    "\n",
    "torch.allclose(update_M(S, U, M, lower_control)[0], torch.tensor(_update_M(_S, _U, _M, lower_control)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _update_U(_S, _U, _M, _C, alpha, lower_control):\n",
    "    \"\"\"Update matrix U.\"\"\"\n",
    "    enum = _S.dot(_M) + alpha * np.dot(_H, _C)\n",
    "    denom = np.dot(_U, np.dot(np.transpose(_M), _M) + alpha * np.dot(np.transpose(_C), _C))\n",
    "    denom[denom < lower_control] = lower_control\n",
    "    _new_U = np.multiply(_U, enum / denom)\n",
    "    row_sums = _U.sum(axis=1)\n",
    "    _new_U = _new_U / row_sums[:, np.newaxis]\n",
    "    return _new_U\n",
    "\n",
    "\n",
    "def update_U(S, U, M, C, alpha, lower_control):\n",
    "    \"\"\"Update matrix U.\"\"\"\n",
    "    enum = S.matmul(M) + alpha * H.matmul(C)\n",
    "    denom = U.matmul(M.transpose(-2, -1).matmul(M) + (alpha * C.transpose(-2, -1).matmul(C)))\n",
    "    denom[denom < lower_control] = lower_control\n",
    "    new_U = U * (enum / denom)\n",
    "    row_sums = U.sum(dim=-1)\n",
    "    new_U = new_U / row_sums.unsqueeze(-1)\n",
    "    return new_U\n",
    "\n",
    "torch.allclose(update_U(S, U, M, C, alpha, lower_control)[0], torch.tensor(_update_U(_S, _U, _M, _C, alpha, lower_control)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _update_C(_U, _H, _C, lower_control):\n",
    "    \"\"\"Update matrix C.\"\"\"\n",
    "    enum = np.dot(np.transpose(_H), _U)\n",
    "    denom = np.dot(_C, np.dot(np.transpose(_U), _U))\n",
    "    denom[denom < lower_control] = lower_control\n",
    "    frac = enum / denom\n",
    "    _C = np.multiply(_C, frac)\n",
    "    row_sums = _C.sum(axis=1)\n",
    "    _C = _C / row_sums[:, np.newaxis]\n",
    "    return _C\n",
    "\n",
    "def update_C(U, H, C, lower_control):\n",
    "    \"\"\"Update matrix C.\"\"\"\n",
    "    enum = H.transpose(-2, -1).matmul(U)\n",
    "    denom = C.matmul(U.transpose(-2, -1).matmul(U))\n",
    "    denom[denom < lower_control] = lower_control\n",
    "    frac = enum / denom\n",
    "    C = C * frac\n",
    "    row_sums = C.sum(dim=-1)\n",
    "    C = C / row_sums.unsqueeze(-1)\n",
    "    return C\n",
    "\n",
    "torch.allclose(update_C(U, H, C, lower_control)[0], torch.tensor(_update_C(_U, _H, _C, lower_control)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _update_H(_B1, _B2, _H, lambd, beta):\n",
    "    \"\"\"Update matrix H.\"\"\"\n",
    "    B1H = _B1.dot(_H)\n",
    "    B2H = _B2.dot(_H)\n",
    "    HHH = np.dot(_H, (np.dot(np.transpose(_H), _H)))\n",
    "    UC = np.dot(_U, np.transpose(_C))\n",
    "    rooted = np.square(2 * beta * B2H) + np.multiply(16 * lambd * HHH, (2 * beta * B1H + 2 * alpha * UC + (4 * lambd - 2 * alpha) * _H))\n",
    "    rooted[rooted < 0] = 0\n",
    "    sqroot_1 = np.sqrt(rooted)\n",
    "    enum = -2 * beta * B2H + sqroot_1\n",
    "    denom = 8 * lambd * HHH\n",
    "    denom[denom < lower_control] = lower_control\n",
    "    rooted = enum / denom\n",
    "    rooted[rooted < 0] = 0\n",
    "    sqroot_2 = np.sqrt(rooted)\n",
    "    _H = np.multiply(_H, sqroot_2)\n",
    "    row_sums = _H.sum(axis=1)\n",
    "    _H = _H / row_sums[:, np.newaxis]\n",
    "    return _H\n",
    "\n",
    "def update_H(B1, B2, H, lambd, beta):\n",
    "    \"\"\"Update matrix H.\"\"\"\n",
    "    B1H = B1.matmul(H)\n",
    "    B2H = B2.matmul(H)\n",
    "    HHH = H.matmul(H.transpose(-2, -1).matmul(H))\n",
    "    UC = U.matmul(C.transpose(-2, -1))\n",
    "    rooted = (2 * beta * B2H).pow(2) + (16 * lambd * HHH * (2 * beta * B1H + 2 * alpha * UC + (4 * lambd - 2 * alpha) * H))\n",
    "    rooted[rooted < 0] = 0\n",
    "    sqroot_1 = rooted.sqrt()\n",
    "    enum = -2 * beta * B2H + sqroot_1\n",
    "    denom = 8 * lambd * HHH\n",
    "    denom[denom < lower_control] = lower_control\n",
    "    rooted = enum / denom\n",
    "    rooted[rooted < 0] = 0\n",
    "    sqroot_2 = rooted.sqrt()\n",
    "    H = H * sqroot_2\n",
    "    row_sums = H.sum(dim=-1)\n",
    "    H = H / row_sums.unsqueeze(-1)\n",
    "    return H\n",
    "\n",
    "torch.allclose(update_H(B1, B2, H, lambd, beta)[0], torch.tensor(_update_H(_B1, _B2, _H, lambd, beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNMF:\n",
    "    \"\"\"Non-negative matrix factorization with modularity maximisation \n",
    "    https://dl.acm.org/doi/10.5555/3298239.3298270\n",
    "    https://github.com/benedekrozemberczki/karateclub/blob/master/karateclub/community_detection/overlapping/mnmf.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions=128, clusters=4, lambd=0.2, alpha=0.05, beta=0.05,\n",
    "                 eta=5.0, iterations=200, epsilon=1e-10, seed=1234):\n",
    "\n",
    "        self.number_of_nodes = number_of_nodes\n",
    "        self.dimensions = dimensions\n",
    "        self.clusters = clusters\n",
    "        self.lambd = lambd\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "        self.seed = seed\n",
    "\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "\n",
    "        batch_size = self.A.shape[0]\n",
    "        number_of_nodes = self.A.shape[-1]\n",
    "        kwargs = dict(dtype=self.A.dtype, device=self.A.device)\n",
    "\n",
    "        # https://stackoverflow.com/questions/50411191/how-to-compute-the-cosine-similarity-in-pytorch-for-all-rows-in-a-matrix-with-re\n",
    "        A_norm = self.A / self.A.norm(dim=-1).unsqueeze(-1)\n",
    "        cos_sim = A_norm.matmul(A_norm.transpose(-2, -1))\n",
    "        \n",
    "        self.M = torch.rand(batch_size, number_of_nodes, self.dimensions, **kwargs)\n",
    "        self.U = torch.rand(batch_size, number_of_nodes, self.dimensions, **kwargs)\n",
    "        self.H = torch.rand(batch_size, number_of_nodes, self.clusters, **kwargs)\n",
    "        self.C = torch.rand(batch_size, self.clusters, self.dimensions, **kwargs)\n",
    "        self.B1 = self._compute_B1()\n",
    "        self.S = self.A + self.eta * cos_sim\n",
    "\n",
    "\n",
    "    def _compute_B1(self):\n",
    "        \"\"\"Calculate modularity matrix.\"\"\"\n",
    "        \n",
    "        degs = self.A.sum(dim=1) + 1\n",
    "        num_edges = self.A.tril().sum(dim=(-2, -1))\n",
    "        batch_size = self.A.shape[0]\n",
    "\n",
    "        edge_indices = torch.stack(self.A.triu().nonzero().transpose(-2, -1)[1:, ...].tensor_split(batch_size, dim=-1))\n",
    "        row_idx = edge_indices[:, 0, :].view(batch_size, -1).expand(-1, -1)\n",
    "        col_idx = edge_indices[:, 1, :].view(batch_size, -1).expand(-1, -1)\n",
    "\n",
    "        scores_l = (torch.gather(degs, -1, row_idx) * torch.gather(degs, -1, col_idx)) / (2 * num_edges.unsqueeze(-1)) \n",
    "        scores_u = (torch.gather(degs, -1, col_idx) * torch.gather(degs, -1, row_idx)) / (2 * num_edges.unsqueeze(-1)) \n",
    "        \n",
    "        B1 = torch.zeros_like(self.A)\n",
    "        B1[:, row_idx, col_idx] = scores_l\n",
    "        B1[:, col_idx, row_idx] = scores_u\n",
    "\n",
    "        return B1\n",
    "\n",
    "\n",
    "    def _update_M(self): \n",
    "        \"\"\"Update matrix M.\"\"\"\n",
    "\n",
    "        enum = self.S.matmul(self.U)\n",
    "        denom = self.M.matmul(self.U.transpose(dim0=-2, dim1=-1).matmul(self.U))\n",
    "        denom[denom < self.epsilon] = self.epsilon\n",
    "        self.M = self.M * (enum / denom)\n",
    "        row_sums = self.M.sum(dim=-1)\n",
    "        self.M = self.M / row_sums.unsqueeze(-1)\n",
    "     \n",
    "\n",
    "    def _update_U(self):\n",
    "        \"\"\"Update matrix U.\"\"\"\n",
    "\n",
    "        enum = self.S.matmul(self.M) + self.alpha * self.H.matmul(self.C)\n",
    "        denom = self.U.matmul(self.M.transpose(dim0=-2, dim1=-1).matmul(self.M) + (self.alpha * self.C.transpose(dim0=-2, dim1=-1).matmul(self.C)))\n",
    "        denom[denom < self.epsilon] = self.epsilon\n",
    "        self.U = self.U * (enum / denom)\n",
    "        row_sums = self.U.sum(dim=-1)\n",
    "        self.U = self.U / row_sums.unsqueeze(-1)\n",
    "  \n",
    "\n",
    "    def _update_C(self):\n",
    "        \"\"\"Update matrix C.\"\"\"\n",
    "\n",
    "        enum = self.H.transpose(dim0=-2, dim1=-1).matmul(self.U)\n",
    "        denom = self.C.matmul(self.U.transpose(dim0=-2, dim1=-1).matmul(self.U))\n",
    "        denom[denom < self.epsilon] = self.epsilon\n",
    "        self.C = self.C * (enum / denom)\n",
    "        row_sums = self.C.sum(dim=-1)\n",
    "        self.C = self.C / row_sums.unsqueeze(-1)\n",
    "\n",
    "\n",
    "    def _update_H(self):\n",
    "        \"\"\"Update matrix H.\"\"\"\n",
    "\n",
    "        AH = self.A.matmul(self.H)\n",
    "        B1H = self.B1.matmul(self.H)\n",
    "        HHH = self.H.matmul(self.H.transpose(dim0=-2, dim1=-1).matmul(self.H))\n",
    "        UC = self.U.matmul(self.C.transpose(dim0=-2, dim1=-1))\n",
    "        \n",
    "        rooted = (2 * self.beta * B1H).pow(2)  \n",
    "        rooted += (16 * self.lambd * HHH * (2 * self.beta * AH + 2 * self.alpha * UC + (4 * self.lambd - 2 * self.alpha) * self.H))\n",
    "        rooted[rooted < 0] = 0\n",
    "        sqroot_1 = rooted.sqrt()\n",
    "        \n",
    "        enum = -2 * self.beta * B1H + sqroot_1\n",
    "        denom = 8 * self.lambd * HHH\n",
    "        denom[denom < self.epsilon] = self.epsilon\n",
    "        \n",
    "        rooted = enum / denom\n",
    "        rooted[rooted < 0] = 0\n",
    "        sqroot_2 = rooted.sqrt()\n",
    "        \n",
    "        self.H = self.H * sqroot_2\n",
    "        row_sums = self.H.sum(dim=-1)\n",
    "        self.H = self.H / row_sums.unsqueeze(-1)\n",
    "\n",
    "    \n",
    "    def _compute_loss(self):\n",
    "\n",
    "        loss = torch.linalg.matrix_norm(self.S - self.M.matmul(self.U.transpose(dim0=-2, dim1=-1))).pow(2)\n",
    "        loss += self.alpha * torch.linalg.matrix_norm(self.H - self.U.matmul(self.C.transpose(dim0=-2, dim1=-1))).pow(2)\n",
    "        loss -= self.beta * ((self.H.transpose(dim0=-2, dim1=-1).matmul((self.A - self.B1))).matmul(self.H)).diagonal(offset=0, dim1=-1, dim2=-2).sum(dim=-1)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def node_community_membership(self):\n",
    "        \"\"\"Get community membership of nodes.\"\"\"\n",
    "\n",
    "        # indices = self.H, axis=1)\n",
    "        # memberships = {i: membership for i, membership in enumerate(indices)}\n",
    "        # return memberships\n",
    "\n",
    "\n",
    "    def node_embedding(self):\n",
    "        \"\"\"Get node embedding.\"\"\"\n",
    "        \n",
    "        embedding = self.U\n",
    "        return embedding\n",
    "\n",
    "\n",
    "    def community_embedding(self):\n",
    "        \"\"\"Get community embedding.\"\"\"\n",
    "\n",
    "        embedding = self.C\n",
    "        return embedding\n",
    "\n",
    "\n",
    "    def fit(self, A):\n",
    "        \"\"\"Fit M-NMF clustering model.\"\"\"\n",
    "\n",
    "        # self._set_seed()\n",
    "\n",
    "        self.A = A\n",
    "\n",
    "        self._initialize_parameters()\n",
    "\n",
    "        for _iter in range(self.iterations):\n",
    "\n",
    "            loss = self._compute_loss()\n",
    "\n",
    "            self._update_M()\n",
    "            self._update_U()\n",
    "            self._update_C()\n",
    "            self._update_H()\n",
    "\n",
    "            print(loss - self._compute_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/data.npz\"\n",
    "data_dic = dict(np.load(data_path, allow_pickle=True).items())\n",
    "A = torch.from_numpy(data_dic[\"A\"]).float()\n",
    "C = torch.from_numpy(data_dic[\"C\"]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 100])\n",
      "torch.Size([1, 100, 4])\n"
     ]
    }
   ],
   "source": [
    "_A = A[0, 1:2, ...]\n",
    "_C = C[0, 1:2, ...]\n",
    "\n",
    "print(_A.shape)\n",
    "print(_C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9678862.])\n",
      "tensor([19.9844])\n",
      "tensor([85.6699])\n",
      "tensor([126.2949])\n",
      "tensor([72.2246])\n",
      "tensor([38.6855])\n",
      "tensor([25.2539])\n",
      "tensor([18.8828])\n",
      "tensor([14.9746])\n",
      "tensor([12.1270])\n",
      "tensor([9.8691])\n",
      "tensor([8.1777])\n",
      "tensor([6.9434])\n",
      "tensor([6.0156])\n",
      "tensor([5.2266])\n",
      "tensor([4.5645])\n",
      "tensor([4.0371])\n",
      "tensor([3.6562])\n",
      "tensor([3.3164])\n",
      "tensor([3.0273])\n",
      "tensor([2.7773])\n",
      "tensor([2.5996])\n",
      "tensor([2.4688])\n",
      "tensor([2.3340])\n",
      "tensor([2.2109])\n",
      "tensor([2.1055])\n",
      "tensor([2.0098])\n",
      "tensor([1.9297])\n",
      "tensor([1.8496])\n",
      "tensor([1.7656])\n",
      "tensor([1.6738])\n",
      "tensor([1.5996])\n",
      "tensor([1.5332])\n",
      "tensor([1.4785])\n",
      "tensor([1.4512])\n",
      "tensor([1.3984])\n",
      "tensor([1.3652])\n",
      "tensor([1.3223])\n",
      "tensor([1.2832])\n",
      "tensor([1.2500])\n",
      "tensor([1.2148])\n",
      "tensor([1.1777])\n",
      "tensor([1.1445])\n",
      "tensor([1.0977])\n",
      "tensor([1.0723])\n",
      "tensor([1.0332])\n",
      "tensor([0.9941])\n",
      "tensor([0.9648])\n",
      "tensor([0.9316])\n",
      "tensor([0.8887])\n",
      "tensor([0.8496])\n",
      "tensor([0.7930])\n",
      "tensor([0.7402])\n",
      "tensor([0.6875])\n",
      "tensor([0.6270])\n",
      "tensor([0.5664])\n",
      "tensor([0.5117])\n",
      "tensor([0.4551])\n",
      "tensor([0.4238])\n",
      "tensor([0.3750])\n",
      "tensor([0.3574])\n",
      "tensor([0.3281])\n",
      "tensor([0.3086])\n",
      "tensor([0.2812])\n",
      "tensor([0.2539])\n",
      "tensor([0.2246])\n",
      "tensor([0.1875])\n",
      "tensor([0.1621])\n",
      "tensor([0.1211])\n",
      "tensor([0.0977])\n",
      "tensor([0.0625])\n",
      "tensor([0.0469])\n",
      "tensor([0.0078])\n",
      "tensor([-0.0039])\n",
      "tensor([-0.0312])\n",
      "tensor([-0.0410])\n",
      "tensor([-0.0703])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0898])\n",
      "tensor([-0.1055])\n",
      "tensor([-0.1191])\n",
      "tensor([-0.1270])\n",
      "tensor([-0.1309])\n",
      "tensor([-0.1406])\n",
      "tensor([-0.1484])\n",
      "tensor([-0.1484])\n",
      "tensor([-0.1562])\n",
      "tensor([-0.1562])\n",
      "tensor([-0.1523])\n",
      "tensor([-0.1602])\n",
      "tensor([-0.1523])\n",
      "tensor([-0.1582])\n",
      "tensor([-0.1484])\n",
      "tensor([-0.1562])\n",
      "tensor([-0.1523])\n",
      "tensor([-0.1504])\n",
      "tensor([-0.1504])\n",
      "tensor([-0.1387])\n",
      "tensor([-0.1426])\n",
      "tensor([-0.1406])\n",
      "tensor([-0.1309])\n",
      "tensor([-0.1289])\n",
      "tensor([-0.1270])\n",
      "tensor([-0.1230])\n",
      "tensor([-0.1152])\n",
      "tensor([-0.1172])\n",
      "tensor([-0.1055])\n",
      "tensor([-0.1094])\n",
      "tensor([-0.1016])\n",
      "tensor([-0.1035])\n",
      "tensor([-0.0938])\n",
      "tensor([-0.0918])\n",
      "tensor([-0.0938])\n",
      "tensor([-0.0879])\n",
      "tensor([-0.0840])\n",
      "tensor([-0.0918])\n",
      "tensor([-0.0781])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0859])\n",
      "tensor([-0.0742])\n",
      "tensor([-0.0840])\n",
      "tensor([-0.0859])\n",
      "tensor([-0.0742])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0801])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0723])\n",
      "tensor([-0.0781])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0781])\n",
      "tensor([-0.0781])\n",
      "tensor([-0.0723])\n",
      "tensor([-0.0781])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0703])\n",
      "tensor([-0.0742])\n",
      "tensor([-0.0684])\n",
      "tensor([-0.0703])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0703])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0703])\n",
      "tensor([-0.0703])\n",
      "tensor([-0.0703])\n",
      "tensor([-0.0703])\n",
      "tensor([-0.0742])\n",
      "tensor([-0.0762])\n",
      "tensor([-0.0742])\n",
      "tensor([-0.0820])\n",
      "tensor([-0.0781])\n",
      "tensor([-0.0859])\n",
      "tensor([-0.0898])\n",
      "tensor([-0.0898])\n",
      "tensor([-0.1016])\n",
      "tensor([-0.0996])\n",
      "tensor([-0.1074])\n",
      "tensor([-0.1152])\n",
      "tensor([-0.1152])\n",
      "tensor([-0.1211])\n",
      "tensor([-0.1309])\n",
      "tensor([-0.1289])\n",
      "tensor([-0.1367])\n",
      "tensor([-0.1504])\n",
      "tensor([-0.1465])\n",
      "tensor([-0.1602])\n",
      "tensor([-0.1621])\n",
      "tensor([-0.1699])\n",
      "tensor([-0.1699])\n",
      "tensor([-0.1758])\n",
      "tensor([-0.1816])\n",
      "tensor([-0.1816])\n",
      "tensor([-0.1855])\n",
      "tensor([-0.1816])\n",
      "tensor([-0.1875])\n",
      "tensor([-0.1797])\n",
      "tensor([-0.1816])\n",
      "tensor([-0.1719])\n",
      "tensor([-0.1758])\n",
      "tensor([-0.1660])\n",
      "tensor([-0.1660])\n",
      "tensor([-0.1621])\n",
      "tensor([-0.1562])\n",
      "tensor([-0.1562])\n",
      "tensor([-0.1504])\n",
      "tensor([-0.1406])\n",
      "tensor([-0.1367])\n",
      "tensor([-0.1309])\n",
      "tensor([-0.1309])\n",
      "tensor([-0.1250])\n",
      "tensor([-0.1211])\n",
      "tensor([-0.1152])\n",
      "tensor([-0.1113])\n",
      "tensor([-0.1152])\n",
      "tensor([-0.1113])\n",
      "tensor([-0.1016])\n",
      "tensor([-0.1055])\n",
      "tensor([-0.1016])\n",
      "tensor([-0.0957])\n"
     ]
    }
   ],
   "source": [
    "model = MNMF()\n",
    "model.fit(_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
       "         3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.H.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "         2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "         3, 3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(_C.argmax(dim=-1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "417ae7846aa939c5468458bfb1f6507418fb70e6e340aeb47010086c1bef9703"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('dynamic-graph-generative-model-vnSGx9wY-python': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
