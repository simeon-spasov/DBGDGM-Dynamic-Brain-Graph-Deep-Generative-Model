{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1807.06560v1.pdf?ref=https://githubhelp.com\n",
    "# https://github.com/renatolfc/chimera-stf/blob/master/chimera/chimera.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_communities = 4\n",
    "num_features = 5\n",
    "community_sizes = [25] * num_communities\n",
    "\n",
    "def community_node_feature_params(num_features, num_communities):\n",
    "    mu = [1, -1] * num_features\n",
    "    mu = list(set(itertools.permutations(mu, num_features)))[:num_communities]\n",
    "\n",
    "    sigma_sq = [0.25, 0.5] * num_features\n",
    "    sigma_sq = list(set(itertools.permutations(sigma_sq, num_features)))[:num_communities]\n",
    "\n",
    "    return mu, sigma_sq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(-1, 1, 1, -1, -1),\n",
       "  (-1, 1, -1, 1, -1),\n",
       "  (1, -1, 1, 1, -1),\n",
       "  (1, -1, -1, -1, -1)],\n",
       " [(0.25, 0.25, 0.5, 0.25, 0.5),\n",
       "  (0.25, 0.25, 0.5, 0.25, 0.25),\n",
       "  (0.25, 0.25, 0.25, 0.5, 0.5),\n",
       "  (0.25, 0.25, 0.25, 0.5, 0.25)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_node_feature_params(num_communities, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BlobDataset(Dataset):\n",
    "     \n",
    "#     def __init__(self, num_samples):\n",
    "        \n",
    "#         centers = [[-1, 1], [1, 1], [-1, -1], [1, -1]]\n",
    "#         features, labels = make_blobs(n_samples=num_samples, centers=centers, cluster_std=0.7, random_state=40)\n",
    "        \n",
    "#         self.features = torch.from_numpy(features).float()\n",
    "#         self.labels = torch.from_numpy(labels).long()\n",
    "     \n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "     \n",
    "#     def __getitem__(self, i):\n",
    "#         return self.features[i], self.labels[i]\n",
    "     \n",
    "#     @property\n",
    "#     def shape(self):\n",
    "#         return (None, 2)\n",
    "\n",
    "\n",
    "def permute_labels(C_pred, C_true):\n",
    "\n",
    "    # check shape\n",
    "    # check one hot\n",
    "\n",
    "    best_acc = 0.\n",
    "    best_perm = tuple()\n",
    "\n",
    "    for perm in list(itertools.permutations(range(C_true.shape[-1]))):\n",
    "        \n",
    "        C_pred_prem = C_pred[..., perm]\n",
    "        acc = ((C_pred_prem == C_true) * 1.).mean().item()\n",
    "\n",
    "        if acc > best_acc:\n",
    "            \n",
    "            best_acc = acc\n",
    "            best_perm = perm\n",
    "\n",
    "    return C_pred[..., best_perm], best_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(nn.Module):\n",
    "    '''k-means clustering using the Lloyd algorithm with Forgy initialization and Euclidean distance'''\n",
    "     \n",
    "    def __init__(self, num_feats, num_clusters, epsilon=1e-10, device=torch.device(\"cpu\"), seed=1234):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_feats = num_feats\n",
    "        self.num_clusters = num_clusters\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    \n",
    "        self.C = torch.zeros(self.num_clusters, self.num_feats).to(device)\n",
    "        self.C_init = False\n",
    "\n",
    "        # self.num_samples_in_clusters = torch.ones(self.num_clusters).to(device)\n",
    "\n",
    "    \n",
    "    def _initialize_C(self, X):\n",
    "        \n",
    "        self.C += X[np.random.choice(len(X), self.num_clusters, replace=False)]\n",
    "        self.C_init = True\n",
    "     \n",
    "\n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        \n",
    "        dist = (x1.unsqueeze(dim=-2) - x2.unsqueeze(dim=-3)).pow(2).sum(dim=-1).squeeze()\n",
    "    \n",
    "        return dist.clamp_min_(self.epsilon)\n",
    "\n",
    "\n",
    "    def _update_C(self, X, cluster):\n",
    "        \n",
    "        matched_cluster, _ = cluster.unique(return_counts=True)\n",
    "        \n",
    "        C = torch.zeros_like(self.C)\n",
    "        mask = (cluster[None].expand(len(matched_cluster), -1) == matched_cluster[:, None]).float()\n",
    "        C[matched_cluster] = mask @ X / mask.sum(dim=-1)[..., :, None]\n",
    "\n",
    "        self.C = C\n",
    "\n",
    "\n",
    "    def _assign_cluster(self, X):\n",
    "\n",
    "        # distance of samples to cluster centroids\n",
    "        distance = self._euclidean_distance(X, self.C)\n",
    "        # assign samples to nearest cluster\n",
    "        cluster = distance.argmin(dim=-1)\n",
    "\n",
    "        return cluster\n",
    "\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "\n",
    "        y_pred = self._assign_cluster(X)\n",
    "        y_pred = F.one_hot(y_pred, num_classes=self.num_clusters)\n",
    "\n",
    "        if y is not None:\n",
    "            y_pred, _ = permute_labels(y_pred , y)\n",
    "\n",
    "        return y_pred\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        if not self.C_init:\n",
    "            self._initialize_C(X)\n",
    "\n",
    "        C_init = self.C\n",
    "\n",
    "        X_cluster = self._assign_cluster(X)\n",
    "        self._update_C(X, X_cluster)\n",
    "\n",
    "        loss = (self.C - C_init).pow(2).sum()\n",
    "\n",
    "        # lr = 1 / self.num_samples_in_clusters[:, None] * 0.9 + 0.1\n",
    "        # self.num_samples_in_clusters[matched_cluster] += count\n",
    "        # print(self.num_samples_in_clusters)\n",
    "        \n",
    "        # self.C = self.C * (1-lr) + C * lr\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 6000\n",
    "# num_clusters = 4\n",
    "# num_epochs = 200\n",
    "# batch_size = 1000\n",
    "\n",
    "# dataset = BlobDataset(num_samples=num_samples)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# num_feats = dataloader.dataset.shape[-1]\n",
    "# kmeans = KMeans(num_feats, num_clusters)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     for X, y in dataloader:\n",
    "\n",
    "#         loss = kmeans(X)\n",
    "\n",
    "#     print(\"epoch {:4d} of {:4d} | loss {:6.4f}\".format(epoch, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = F.one_hot(dataset.labels, num_classes=num_clusters)\n",
    "# # distance of samples to cluster centroids\n",
    "# y_pred = kmeans.predict(dataset.features, y)\n",
    "\n",
    "# acc = ((y_pred.argmax(-1) == y.argmax(-1)) * 1.).mean().item() * 100\n",
    "\n",
    "# print(round(acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMF(nn.Module):\n",
    "    '''...'''\n",
    "     \n",
    "    def __init__(self, num_samples, num_nodes, num_feats, time_len, latent_dim, beta=1., lambda1=1., lambda2=1., device=torch.device(\"cpu\")):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.U = nn.Parameter(torch.rand(num_samples, time_len, num_nodes, latent_dim)).to(device)\n",
    "        self.V = nn.Parameter(torch.rand(num_samples, num_nodes, latent_dim)).to(device)\n",
    "        self.W = nn.Parameter(torch.rand(num_samples, num_feats, latent_dim)).to(device)\n",
    "\n",
    "        self.beta = beta\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "\n",
    "\n",
    "    def forward(self, A, X, index=None):\n",
    "\n",
    "        if index is None:\n",
    "            index = list(range(A.shape[0]))\n",
    "\n",
    "        U = self.U[index, ...].clamp_min(0.)\n",
    "        V = self.V[index, None, ...].clamp_min(0.)\n",
    "        W = self.W[index, None, ...].clamp_min(0.)\n",
    "\n",
    "        error_A = (A - U.matmul(V.transpose(-2, -1))).pow(2).sum((-3, -2, -1))\n",
    "        error_X = (X - U.matmul(W.transpose(-2, -1))).pow(2).sum((-3, -2, -1))\n",
    "        \n",
    "        norm_V = V.pow(2).sum((-3, -2, -1))\n",
    "        norm_W = W.pow(2).sum((-3, -2, -1))\n",
    "        norm_U = U.pow(2).sum((-3, -2, -1))\n",
    "        norm_U_diff = U.diff(n=1, dim=-3).sum((-3, -2, -1))\n",
    "\n",
    "        loss = error_A + (self.beta * error_X) + self.lambda1 * (norm_V + norm_W + norm_U) + (self.lambda2 * norm_U_diff)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/data.npz\"\n",
    "\n",
    "data_dic = dict(np.load(data_path, allow_pickle=True).items())\n",
    "dataset = torch.utils.data.TensorDataset(torch.from_numpy(data_dic[\"A\"]).float())\n",
    "\n",
    "num_samples, time_len, num_nodes, _ = dataset.tensors[0].shape\n",
    "num_feats = num_nodes\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=num_samples, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10, 100, 100])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chimera(nn.Module):\n",
    "    '''...'''\n",
    "\n",
    "    def __init__(self, num_samples, num_nodes, num_feats, time_len, num_communities, latent_dim, beta=1., lambda1=1., lambda2=1., device=torch.device(\"cpu\")):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_feats = num_feats\n",
    "        self.time_len = time_len\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.nmf = NMF(num_samples, num_nodes, num_feats, time_len, latent_dim, beta, lambda1, lambda2, device)\n",
    "        self.k_means = KMeans(latent_dim, num_communities, device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chimera = Chimera()\n",
    "\n",
    "optimiszer = ()\n",
    "\n",
    "for X in dataloader:\n",
    "    X = X.to(device)\n",
    "    loss = chimera.nmf(X)\n",
    "\n",
    "node_embeddings = chimera.nmf.U\n",
    "del chimera.nmf\n",
    "\n",
    "node_embeddings = node_embeddings.reshape(-1, latent_dim)\n",
    "dataloader = DataLoader(node_embeddings, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for U in dataloader:\n",
    "    U = U.to(device)\n",
    "    loss = chimera.k_means(U)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([554.8316, 582.7816, 623.6739], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 7\n",
    "time_len = 11\n",
    "num_nodes = 10\n",
    "num_feats = 5 \n",
    "num_communities = 3\n",
    "\n",
    "A = torch.rand(num_samples, time_len, num_nodes, num_nodes)\n",
    "X = torch.rand(num_samples, time_len, num_nodes, num_feats)\n",
    "\n",
    "chimera = Chimera(num_samples, num_nodes, num_feats, time_len, num_communities)\n",
    "\n",
    "chimera.matrix_factorization_loss(A[[0, 1, 3], ...], X[[0, 1, 3], ...], index=[0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "417ae7846aa939c5468458bfb1f6507418fb70e6e340aeb47010086c1bef9703"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('dynamic-graph-generative-model-vnSGx9wY-python': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
