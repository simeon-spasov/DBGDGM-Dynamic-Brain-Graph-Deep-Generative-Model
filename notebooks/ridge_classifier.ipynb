{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow imports from parent directory\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.dataset import _get_filepaths, _load_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: hcp | thresh:   0 | time:   50 | run:  1 | acc: 61.7%\n",
      "dataset: hcp | thresh:   0 | time:   50 | run:  2 | acc: 56.7%\n",
      "dataset: hcp | thresh:   0 | time:   50 | run:  3 | acc: 66.7%\n",
      "dataset: hcp | thresh:   0 | time:   50 | run:  4 | acc: 61.7%\n",
      "dataset: hcp | thresh:   0 | time:   50 | run:  5 | acc: 63.3%\n",
      "dataset: hcp | thresh:   0 | time:  100 | run:  1 | acc: 80.0%\n",
      "dataset: hcp | thresh:   0 | time:  100 | run:  2 | acc: 66.7%\n",
      "dataset: hcp | thresh:   0 | time:  100 | run:  3 | acc: 73.3%\n",
      "dataset: hcp | thresh:   0 | time:  100 | run:  4 | acc: 78.3%\n",
      "dataset: hcp | thresh:   0 | time:  100 | run:  5 | acc: 70.0%\n",
      "dataset: hcp | thresh:   0 | time:  200 | run:  1 | acc: 80.0%\n",
      "dataset: hcp | thresh:   0 | time:  200 | run:  2 | acc: 78.3%\n",
      "dataset: hcp | thresh:   0 | time:  200 | run:  3 | acc: 76.7%\n",
      "dataset: hcp | thresh:   0 | time:  200 | run:  4 | acc: 80.0%\n",
      "dataset: hcp | thresh:   0 | time:  200 | run:  5 | acc: 68.3%\n",
      "dataset: hcp | thresh:   0 | time:  600 | run:  1 | acc: 75.0%\n",
      "dataset: hcp | thresh:   0 | time:  600 | run:  2 | acc: 80.0%\n",
      "dataset: hcp | thresh:   0 | time:  600 | run:  3 | acc: 81.7%\n",
      "dataset: hcp | thresh:   0 | time:  600 | run:  4 | acc: 85.0%\n",
      "dataset: hcp | thresh:   0 | time:  600 | run:  5 | acc: 73.3%\n",
      "dataset: hcp | thresh:   0 | time: 1200 | run:  1 | acc: 76.7%\n",
      "dataset: hcp | thresh:   0 | time: 1200 | run:  2 | acc: 75.0%\n",
      "dataset: hcp | thresh:   0 | time: 1200 | run:  3 | acc: 81.7%\n",
      "dataset: hcp | thresh:   0 | time: 1200 | run:  4 | acc: 76.7%\n",
      "dataset: hcp | thresh:   0 | time: 1200 | run:  5 | acc: 80.0%\n",
      "dataset: hcp | thresh:   5 | time:   50 | run:  1 | acc: 58.3%\n",
      "dataset: hcp | thresh:   5 | time:   50 | run:  2 | acc: 65.0%\n",
      "dataset: hcp | thresh:   5 | time:   50 | run:  3 | acc: 61.7%\n",
      "dataset: hcp | thresh:   5 | time:   50 | run:  4 | acc: 55.0%\n",
      "dataset: hcp | thresh:   5 | time:   50 | run:  5 | acc: 48.3%\n",
      "dataset: hcp | thresh:   5 | time:  100 | run:  1 | acc: 60.0%\n",
      "dataset: hcp | thresh:   5 | time:  100 | run:  2 | acc: 65.0%\n",
      "dataset: hcp | thresh:   5 | time:  100 | run:  3 | acc: 63.3%\n",
      "dataset: hcp | thresh:   5 | time:  100 | run:  4 | acc: 73.3%\n",
      "dataset: hcp | thresh:   5 | time:  100 | run:  5 | acc: 50.0%\n",
      "dataset: hcp | thresh:   5 | time:  200 | run:  1 | acc: 65.0%\n",
      "dataset: hcp | thresh:   5 | time:  200 | run:  2 | acc: 71.7%\n",
      "dataset: hcp | thresh:   5 | time:  200 | run:  3 | acc: 70.0%\n",
      "dataset: hcp | thresh:   5 | time:  200 | run:  4 | acc: 76.7%\n",
      "dataset: hcp | thresh:   5 | time:  200 | run:  5 | acc: 58.3%\n",
      "dataset: hcp | thresh:   5 | time:  600 | run:  1 | acc: 73.3%\n",
      "dataset: hcp | thresh:   5 | time:  600 | run:  2 | acc: 78.3%\n",
      "dataset: hcp | thresh:   5 | time:  600 | run:  3 | acc: 78.3%\n",
      "dataset: hcp | thresh:   5 | time:  600 | run:  4 | acc: 70.0%\n",
      "dataset: hcp | thresh:   5 | time:  600 | run:  5 | acc: 65.0%\n",
      "dataset: hcp | thresh:   5 | time: 1200 | run:  1 | acc: 75.0%\n",
      "dataset: hcp | thresh:   5 | time: 1200 | run:  2 | acc: 68.3%\n",
      "dataset: hcp | thresh:   5 | time: 1200 | run:  3 | acc: 73.3%\n",
      "dataset: hcp | thresh:   5 | time: 1200 | run:  4 | acc: 58.3%\n",
      "dataset: hcp | thresh:   5 | time: 1200 | run:  5 | acc: 61.7%\n",
      "dataset: hcp | thresh:  10 | time:   50 | run:  1 | acc: 56.7%\n",
      "dataset: hcp | thresh:  10 | time:   50 | run:  2 | acc: 56.7%\n",
      "dataset: hcp | thresh:  10 | time:   50 | run:  3 | acc: 60.0%\n",
      "dataset: hcp | thresh:  10 | time:   50 | run:  4 | acc: 53.3%\n",
      "dataset: hcp | thresh:  10 | time:   50 | run:  5 | acc: 45.0%\n",
      "dataset: hcp | thresh:  10 | time:  100 | run:  1 | acc: 66.7%\n",
      "dataset: hcp | thresh:  10 | time:  100 | run:  2 | acc: 68.3%\n",
      "dataset: hcp | thresh:  10 | time:  100 | run:  3 | acc: 65.0%\n",
      "dataset: hcp | thresh:  10 | time:  100 | run:  4 | acc: 65.0%\n",
      "dataset: hcp | thresh:  10 | time:  100 | run:  5 | acc: 48.3%\n",
      "dataset: hcp | thresh:  10 | time:  200 | run:  1 | acc: 66.7%\n",
      "dataset: hcp | thresh:  10 | time:  200 | run:  2 | acc: 71.7%\n",
      "dataset: hcp | thresh:  10 | time:  200 | run:  3 | acc: 73.3%\n",
      "dataset: hcp | thresh:  10 | time:  200 | run:  4 | acc: 73.3%\n",
      "dataset: hcp | thresh:  10 | time:  200 | run:  5 | acc: 55.0%\n",
      "dataset: hcp | thresh:  10 | time:  600 | run:  1 | acc: 73.3%\n",
      "dataset: hcp | thresh:  10 | time:  600 | run:  2 | acc: 73.3%\n",
      "dataset: hcp | thresh:  10 | time:  600 | run:  3 | acc: 71.7%\n",
      "dataset: hcp | thresh:  10 | time:  600 | run:  4 | acc: 71.7%\n",
      "dataset: hcp | thresh:  10 | time:  600 | run:  5 | acc: 66.7%\n",
      "dataset: hcp | thresh:  10 | time: 1200 | run:  1 | acc: 76.7%\n",
      "dataset: hcp | thresh:  10 | time: 1200 | run:  2 | acc: 71.7%\n",
      "dataset: hcp | thresh:  10 | time: 1200 | run:  3 | acc: 70.0%\n",
      "dataset: hcp | thresh:  10 | time: 1200 | run:  4 | acc: 60.0%\n",
      "dataset: hcp | thresh:  10 | time: 1200 | run:  5 | acc: 71.7%\n",
      "dataset: hcp | thresh:  25 | time:   50 | run:  1 | acc: 61.7%\n",
      "dataset: hcp | thresh:  25 | time:   50 | run:  2 | acc: 56.7%\n",
      "dataset: hcp | thresh:  25 | time:   50 | run:  3 | acc: 73.3%\n",
      "dataset: hcp | thresh:  25 | time:   50 | run:  4 | acc: 56.7%\n",
      "dataset: hcp | thresh:  25 | time:   50 | run:  5 | acc: 51.7%\n",
      "dataset: hcp | thresh:  25 | time:  100 | run:  1 | acc: 71.7%\n",
      "dataset: hcp | thresh:  25 | time:  100 | run:  2 | acc: 61.7%\n",
      "dataset: hcp | thresh:  25 | time:  100 | run:  3 | acc: 63.3%\n",
      "dataset: hcp | thresh:  25 | time:  100 | run:  4 | acc: 73.3%\n",
      "dataset: hcp | thresh:  25 | time:  100 | run:  5 | acc: 58.3%\n",
      "dataset: hcp | thresh:  25 | time:  200 | run:  1 | acc: 68.3%\n",
      "dataset: hcp | thresh:  25 | time:  200 | run:  2 | acc: 70.0%\n",
      "dataset: hcp | thresh:  25 | time:  200 | run:  3 | acc: 76.7%\n",
      "dataset: hcp | thresh:  25 | time:  200 | run:  4 | acc: 78.3%\n",
      "dataset: hcp | thresh:  25 | time:  200 | run:  5 | acc: 56.7%\n",
      "dataset: hcp | thresh:  25 | time:  600 | run:  1 | acc: 73.3%\n",
      "dataset: hcp | thresh:  25 | time:  600 | run:  2 | acc: 76.7%\n",
      "dataset: hcp | thresh:  25 | time:  600 | run:  3 | acc: 73.3%\n",
      "dataset: hcp | thresh:  25 | time:  600 | run:  4 | acc: 75.0%\n",
      "dataset: hcp | thresh:  25 | time:  600 | run:  5 | acc: 70.0%\n",
      "dataset: hcp | thresh:  25 | time: 1200 | run:  1 | acc: 78.3%\n",
      "dataset: hcp | thresh:  25 | time: 1200 | run:  2 | acc: 70.0%\n",
      "dataset: hcp | thresh:  25 | time: 1200 | run:  3 | acc: 76.7%\n",
      "dataset: hcp | thresh:  25 | time: 1200 | run:  4 | acc: 58.3%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m fpath \u001b[39min\u001b[39;00m fpaths:    \n\u001b[1;32m     22\u001b[0m     _X, _y \u001b[39m=\u001b[39m _load_timeseries(fpath, zscore\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m     fcm \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mfit_transform([_X[:, :t]\u001b[39m.\u001b[39;49mT])[\u001b[39m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m     np\u001b[39m.\u001b[39mfill_diagonal(fcm, \u001b[39m0.\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m0.\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dynamic-graph-generative-model-Ou1M7RSW/lib/python3.9/site-packages/nilearn/connectome/connectivity_matrices.py:569\u001b[0m, in \u001b[0;36mConnectivityMeasure.fit_transform\u001b[0;34m(self, X, y, confounds)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(X) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    565\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTangent space parametrization can only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbe applied to a group of subjects, as it returns \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdeviations to the mean. You provided \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m X\n\u001b[1;32m    568\u001b[0m             )\n\u001b[0;32m--> 569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, do_fit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, do_transform\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    570\u001b[0m                            confounds\u001b[39m=\u001b[39;49mconfounds)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dynamic-graph-generative-model-Ou1M7RSW/lib/python3.9/site-packages/nilearn/connectome/connectivity_matrices.py:473\u001b[0m, in \u001b[0;36mConnectivityMeasure._fit_transform\u001b[0;34m(self, X, do_transform, do_fit, confounds)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m# Compute all the matrices, stored in \"connectivities\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcorrelation\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 473\u001b[0m     covariances_std \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_estimator_\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    474\u001b[0m         signal\u001b[39m.\u001b[39m_standardize(x, detrend\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, standardize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    475\u001b[0m         )\u001b[39m.\u001b[39mcovariance_ \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m    476\u001b[0m     connectivities \u001b[39m=\u001b[39m [cov_to_corr(cov) \u001b[39mfor\u001b[39;00m cov \u001b[39min\u001b[39;00m covariances_std]\n\u001b[1;32m    477\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dynamic-graph-generative-model-Ou1M7RSW/lib/python3.9/site-packages/nilearn/connectome/connectivity_matrices.py:473\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m# Compute all the matrices, stored in \"connectivities\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcorrelation\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 473\u001b[0m     covariances_std \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcov_estimator_\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    474\u001b[0m         signal\u001b[39m.\u001b[39;49m_standardize(x, detrend\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, standardize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    475\u001b[0m         )\u001b[39m.\u001b[39mcovariance_ \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m    476\u001b[0m     connectivities \u001b[39m=\u001b[39m [cov_to_corr(cov) \u001b[39mfor\u001b[39;00m cov \u001b[39min\u001b[39;00m covariances_std]\n\u001b[1;32m    477\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dynamic-graph-generative-model-Ou1M7RSW/lib/python3.9/site-packages/sklearn/covariance/_shrunk_covariance.py:478\u001b[0m, in \u001b[0;36mLedoitWolf.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n\u001b[1;32m    477\u001b[0m \u001b[39mwith\u001b[39;00m config_context(assume_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 478\u001b[0m     covariance, shrinkage \u001b[39m=\u001b[39m ledoit_wolf(\n\u001b[1;32m    479\u001b[0m         X \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocation_, assume_centered\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, block_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size\n\u001b[1;32m    480\u001b[0m     )\n\u001b[1;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshrinkage_ \u001b[39m=\u001b[39m shrinkage\n\u001b[1;32m    482\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_covariance(covariance)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dynamic-graph-generative-model-Ou1M7RSW/lib/python3.9/site-packages/sklearn/covariance/_shrunk_covariance.py:339\u001b[0m, in \u001b[0;36mledoit_wolf\u001b[0;34m(X, assume_centered, block_size)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39m# get Ledoit-Wolf shrinkage\u001b[39;00m\n\u001b[1;32m    336\u001b[0m shrinkage \u001b[39m=\u001b[39m ledoit_wolf_shrinkage(\n\u001b[1;32m    337\u001b[0m     X, assume_centered\u001b[39m=\u001b[39massume_centered, block_size\u001b[39m=\u001b[39mblock_size\n\u001b[1;32m    338\u001b[0m )\n\u001b[0;32m--> 339\u001b[0m emp_cov \u001b[39m=\u001b[39m empirical_covariance(X, assume_centered\u001b[39m=\u001b[39;49massume_centered)\n\u001b[1;32m    340\u001b[0m mu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mtrace(emp_cov)) \u001b[39m/\u001b[39m n_features\n\u001b[1;32m    341\u001b[0m shrunk_cov \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m shrinkage) \u001b[39m*\u001b[39m emp_cov\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/dynamic-graph-generative-model-Ou1M7RSW/lib/python3.9/site-packages/sklearn/covariance/_empirical_covariance.py:91\u001b[0m, in \u001b[0;36mempirical_covariance\u001b[0;34m(X, assume_centered)\u001b[0m\n\u001b[1;32m     86\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one sample available. You may want to reshape your data array\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m assume_centered:\n\u001b[0;32m---> 91\u001b[0m     covariance \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X\u001b[39m.\u001b[39;49mT, X) \u001b[39m/\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     covariance \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcov(X\u001b[39m.\u001b[39mT, bias\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 12345\n",
    "num_runs = 5\n",
    "test_prop = 0.2\n",
    "\n",
    "alpha = 1.\n",
    "measure = \"correlation\" # \"partial correlation\"  \"precision\"\n",
    "top_percent = [5, 10, 25, 50, 80, 100]\n",
    "max_timepoint = [50, 100, 200, 600, 1200]\n",
    "\n",
    "results = []\n",
    "for dataset, fpaths in dict(hcp=_get_filepaths(dataset=\"hcp\", data_dir=\"../data\"),\n",
    "                            ukb=_get_filepaths(dataset=\"ukb\", data_dir=\"../data\")).items():\n",
    "    for p in top_percent:\n",
    "        for t in max_timepoint:\n",
    "            if (dataset == \"ukb\") and (t > 600):\n",
    "                break\n",
    "            for run in range(1, num_runs + 1):\n",
    "                model = RidgeClassifier(alpha=alpha, class_weight=\"balanced\", solver=\"auto\")\n",
    "                conn = ConnectivityMeasure(kind=measure, vectorize=False, discard_diagonal=False)\n",
    "                X, y = [], []\n",
    "                for fpath in fpaths:    \n",
    "                    _X, _y = _load_timeseries(fpath, zscore=True)\n",
    "                    fcm = conn.fit_transform([_X[:, :t].T])[0]\n",
    "                    np.fill_diagonal(fcm, 0.)\n",
    "                    if p < 100.:\n",
    "                        threshold = np.percentile(fcm.flatten(), 100. - p)\n",
    "                        fcm[fcm < threshold] = 0.\n",
    "                    X += [fcm[np.tril_indices(fcm.shape[-1])]]\n",
    "                    y += [_y]\n",
    "                X = np.array(X)\n",
    "                y = np.array(y)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_prop, random_state=seed + run)\n",
    "                model.fit(X_train, y_train)\n",
    "                acc = model.score(X_test, y_test) \n",
    "                results += [[dataset, p, t, run, acc]]\n",
    "                print(\"dataset: {} | thresh: {:3d} | time: {:4d} | run: {:2d} | acc: {:04.1f}%\".format(dataset, p, t, run, acc * 100))\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.columns = [\"dataset\", \"thresh\", \"max_time\", \"run\", \"acc\"]\n",
    "df.groupby([\"dataset\", \"thresh\", \"max_time\"], as_index=False).agg({\"acc\": [\"mean\", \"std\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('dynamic-graph-generative-model-Ou1M7RSW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e334c366cf7f6e87f5fe935940c7d6751f57c6438059c6cc189cf0b95ac4eb2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
